In this paper, we have explored the possibilites of a reputation based proof-of-work protocol in mitigating DoS attacks. We have presented the RB-PoW protocol, an experimental architecture and tested the protocol with a web-based simulation interface. Furthermore, we presented results of two simulation experiments, server flooding and server draining with two types of implementations; classical proof-of-work and the proposed reputation based proof-of-work. The results from these simulation experiments will be brought to discussion in following paragraphs with the goal of answering the initial problem statements:
\\
\\
\\
\textbf{Server flooding}
\vspace{7pt}
\\
The results of simulating server flooding, see table \ref{tab:flooding}, shows that the RB-PoW protocol is a vast improvement to mitigating DoS flooding attacks in comparision the the classical PoW. It is interesting to note that the RB-PoW perform better in all three user-type cases. The most significant improvement was to the mobile users where the service time was improved by approximately 900\%. 

This finding was unexpected and suggests that PoW in its classical implementation is very primitive.
There are several possible explanations for this result and the most orenous reason is likely to be the fact that the classical PoW does not in any way seperate legitimate users from adversaries.
There are, however, other possible explanations. It may in fact be the result of inadequate tuning of the PoW protocol. 
\\
\\
\textbf{Server draining}
\vspace{7pt}
\\
Contrary to expectations, the results of simulating server draining in table \ref{tab:draining} show that classical PoW is slightly better at servicing legitimate users with a service time \% better than RB-PoW. However, RB-PoW still performs significantly better than classical PoW regarding mobile users, with a \% faster service time. Although, the performance of servicing mobile users in both protocols would in a real world context be far from acceptable.

Even though the results was rather contradictory to our expectation the explaination for this result is rather reasonable. The proposed RB-PoW system is based on differentiating malicious behaviour from legitimate behaviour. However, in this simulation experiment the adversaries are programmed to fake themselves as legitimate user,  




\subsection{Feasibility}
\subsection{Statistical Confidence}
An important question of our study is, \emph{when do we trust our data?} The results presented in Section \ref{result} was collected from the simulation experiments run through the web-based simulation interface. The data was collected as samples during a certain time-frame during the experiments, dividing data into the categories of \emph{adversaries}, \emph{legitimate users} and \emph{mobile device users}. However,  can we be certain that the calculated averages of our samples represent the average of the whole population?

The answer lies in statistical mathematics. To bring confidence in the presented results it is important that a average of the sample data can, with a certain propability, be found within a interval of confidence, also known as confidence interval. Hence, knowing this interval enables the possiblity to infer that the average of one sample is significantly different from another, as long as the confidence interval of the averages do not overlap.

One fundamental requirement of finding these confidence intervals is that the distrubution of the samples is known. However, the distribution of our the result data in our simulation experiments is likely a sum of pascal and unknown distributed variables. Because of the uncertainty a bit of magic is required to solve this problem. 

\subsubsection{Arithmetic Averages Have a Bit of Magic}
That bit of magic is the \emph{Central Limit Theorem}. The theorem states that, given a sufficiently large sample of indentically distributed independent variables,\footnote{The central limit theorem generally takes effect when samples is larger than 30.}, each with a well-defined mean and well-defined variance\footnote{This implies that both the mean the variance should be finite.}, will be approximately normally distributed\cite{gunnar}.

From this theorem two implications can be drawn that is relevant for our test data:
\begin{itemize}
\item A random sample scan be taken from any population, in our case samples of the simulation experiments, even if the simulation data is not normally distributed and assume it to be approximately normally distributed.

\item The theorem also allows assumptions to be made about the sample data of the simulation regardless of the entire simulation data. Thereby, an interval estimation can be made about the true average of the simulation data with only sample data.
\end{itemize}

 \subsubsection{Confidence in Test Results}
The probability that the average of whole population falls within the interval of the result data is either 1 or 0 - the interval captures the average or it doesn't\cite{gunnar}. However, with a 99\% confidence it is safe to assume that the average of the whole population actually falls within the confidence interval of the result data. 


\subsection{Lessons learned}
\subsection{Suggested Directions for Future Research}

In future investigations it might be possible to use a different X in which ...... 

Further studies, which take these variables into account, will need to be undertaken.
